{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from PyPDF2 import PdfReader\n",
    "import json\n",
    "import string\n",
    "from models import GPT_Model\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_choices = [\"forward\",\"backwards\",\"turning\",\"grooming\",\"oviposition\"]\n",
    "modality_choices = [\"olfactory\", \"hygrosensory\",\"thermosensory\",\"visual\"]\n",
    "#modality_choices = [\"olfactory\",\"thermohygro\", \"hygrosensory\",\"thermosensory\",\"walking\",'flight',\"grooming\",\"landing\",\"escape_takeoff\",\"reproduction\",\"neuropeptidergic\",\"neuromodulatory\",\"foreleg\",\"turning\",\"rest\",\"puff\",\"halting\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MBON_types = ['MBON01','MBON02','MBON03','MBON04','MBON05','MBON06','MBON07','MBON09','MBON10','MBON11','MBON12','MBON13','MBON14','MBON15','MBON15-like','MBON16','MBON17','MBON17-like','MBON18','MBON19','MBON20','MBON21','MBON22','MBON23','MBON24','MBON25','MBON26','MBON27','MBON28','MBON29','MBON30','MBON31','MBON32','MBON33','MBON34','MBON35']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_from_entities(entities, modality_choices):\n",
    "    \"\"\"\n",
    "    Generate questions for a list of entities with labeled multiple-choice options.\n",
    "\n",
    "    Parameters:\n",
    "    - entities (list): List of entities to generate questions for.\n",
    "    - modality_choices (list): List of sensory modalities.\n",
    "\n",
    "    Returns:\n",
    "    - questions (list): List of questions in JSON format.\n",
    "    \"\"\"\n",
    "    # Assign labels (A, B, C, ...) to choices\n",
    "    labeled_choices = [f\"{label}. {choice}\" for label, choice in zip(string.ascii_uppercase, modality_choices + [\"modality information not described\"])]\n",
    "\n",
    "    questions = []\n",
    "    for entity in entities:\n",
    "        # First question: Is the entity mentioned in this chunk?\n",
    "        mention_question = {\n",
    "            \"entity\": entity,\n",
    "            \"question\": f\"Is the entity '{entity}' mentioned or discussed in this chunk? Answer only in Yes or No. Answer:\",\n",
    "            \"answer\": \"\"  # To be filled after processing\n",
    "        }\n",
    "        #questions.append(mention_question)\n",
    "        \n",
    "        # Second question: If mentioned, what sensory modality is it associated with?\n",
    "        modality_question = {\n",
    "            \"entity\": entity,\n",
    "            \"question\": f\"Given the paper chunk, what sensory modality is the entity '{entity}' associated with? Choose from:\\n\" + \"\\n\".join(labeled_choices)+\". \\nAnswer only in the form of the letter. Answer:\",\n",
    "            \"answer\": \"\"  # To be filled after processing\n",
    "        }\n",
    "        summary_question = {\n",
    "            \"entity\": entity,\n",
    "            \"question\": f\"Given the paper chunk, summarize key information related to '{entity}'. Answer:\",\n",
    "            \"answer\": \"\"  # To be filled after processing\n",
    "        }\n",
    "        #questions.append(modality_question)\n",
    "        questions.append(summary_question)\n",
    "    \n",
    "    return questions\n",
    "\n",
    "def save_questions_to_json(questions, output_file):\n",
    "    \"\"\"\n",
    "    Save generated questions to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - questions (list): List of questions to save.\n",
    "    - output_file (str): Path to the output JSON file.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as file:\n",
    "        json.dump(questions, file, indent=4)\n",
    "    print(f\"Questions saved to {output_file}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions saved to questions/mbon_questions.json.\n"
     ]
    }
   ],
   "source": [
    "# Generate questions\n",
    "questions = generate_questions_from_entities(MBON_types, modality_choices)\n",
    "\n",
    "# Save questions to JSON\n",
    "output_json_path = \"questions/mbon_questions.json\"\n",
    "save_questions_to_json(questions, output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions saved to questions/mbon_summary.json.\n"
     ]
    }
   ],
   "source": [
    "# Generate questions\n",
    "questions = generate_questions_from_entities(MBON_types, modality_choices)\n",
    "\n",
    "# Save questions to JSON\n",
    "output_json_path = \"questions/mbon_summary.json\"\n",
    "save_questions_to_json(questions, output_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, max_tokens=120000):\n",
    "    \"\"\"Split text into chunks that fit within the token limit.\"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) + 1 > max_tokens:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence + \". \"\n",
    "        else:\n",
    "            current_chunk += sentence + \". \"\n",
    "    if current_chunk:  # Append the remaining chunk\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def ask_questions_and_fill_answers(chunks, questions_file, gpt_model):\n",
    "    \"\"\"\n",
    "    Process each chunk to answer all questions efficiently, and fill the answers in the same JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - chunks (list): List of text chunks from the paper.\n",
    "    - questions_file (str): Path to the JSON file with questions.\n",
    "    - gpt_model (GPT_Model): An instance of the GPT_Model class.\n",
    "\n",
    "    Returns:\n",
    "    - None: Updates the JSON file in place.\n",
    "    \"\"\"\n",
    "    # Load questions from the JSON file\n",
    "    with open(questions_file, \"r\") as file:\n",
    "        questions = json.load(file)\n",
    "\n",
    "    # Process each chunk\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {i + 1}/{len(chunks)}...\")\n",
    "\n",
    "        # Iterate through each question\n",
    "        for question_dict in questions:\n",
    "            question = question_dict.get(\"question\")\n",
    "            if not question or question_dict.get(\"answer\"):  # Skip if already answered\n",
    "                continue\n",
    "\n",
    "            # Construct the prompt for the question\n",
    "            prompt = f\"\"\"\n",
    "            The following is a section from a research paper:\n",
    "\n",
    "            {chunk}\n",
    "\n",
    "            Based on this section, answer the following question:\n",
    "\n",
    "            Q: {question}\n",
    "            \"\"\"\n",
    "\n",
    "            # Use GPT_Model to get the response\n",
    "            response = gpt_model.get_response(prompt)\n",
    "\n",
    "            # If a valid response is returned, update the answer field\n",
    "            if response and response.strip():\n",
    "                # If it's the first question asking about mentioning the entity, check the response\n",
    "                if \"mentioned\" in question.lower():\n",
    "                    if \"no\" in response.lower():\n",
    "                        # Skip further questions for this entity if it's not mentioned\n",
    "                        break\n",
    "\n",
    "                question_dict[\"answer\"] = response.strip()\n",
    "\n",
    "    # Save the updated questions back to the same JSON file\n",
    "    with open(questions_file, \"w\") as file:\n",
    "        json.dump(questions, file, indent=4)\n",
    "\n",
    "    print(f\"Answers have been filled and saved back to {questions_file}.\")\n",
    "\n",
    "def ask_questions_and_fill_answers_2(chunks, questions_file, gpt_model):\n",
    "    \"\"\"\n",
    "    Process each chunk once to ask all questions efficiently, and append answers to the existing ones in the same JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - chunks (list): List of text chunks from the paper.\n",
    "    - questions_file (str): Path to the JSON file with questions.\n",
    "    - gpt_model (GPT_Model): An instance of the GPT_Model class.\n",
    "\n",
    "    Returns:\n",
    "    - None: Updates the JSON file in place.\n",
    "    \"\"\"\n",
    "    # Load questions from the JSON file\n",
    "    with open(questions_file, \"r\") as file:\n",
    "        questions = json.load(file)\n",
    "\n",
    "    # Process each chunk\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {i + 1}/{len(chunks)}...\")\n",
    "\n",
    "        # Prepare a single prompt with all questions\n",
    "        prompt = f\"\"\"\n",
    "        The following is a section from a research paper:\n",
    "\n",
    "        {chunk}\n",
    "\n",
    "        Based on this section, answer the following questions:\n",
    "        \"\"\"\n",
    "        for idx, question_dict in enumerate(questions, 1):\n",
    "            prompt += f\"\\n{idx}. {question_dict['question']}\"\n",
    "\n",
    "        prompt += \"\\n\\nProvide your answers in the format:\\n1. [Answer to question 1]\\n2. [Answer to question 2]\\n...\"\n",
    "\n",
    "        # Get the response from GPT\n",
    "        response = gpt_model.get_response(prompt)\n",
    "\n",
    "        # Parse the response and append answers\n",
    "        if response:\n",
    "            try:\n",
    "                lines = response.split(\"\\n\")\n",
    "                for line in lines:\n",
    "                    # Match format: \"1. [Answer]\"\n",
    "                    if \". \" in line:\n",
    "                        idx, answer = line.split(\". \", 1)\n",
    "                        idx = int(idx) - 1  # Convert to 0-based index\n",
    "                        if idx < len(questions):\n",
    "                            # Append the new answer to the existing answer\n",
    "                            existing_answer = questions[idx].get(\"answer\", \"\").strip()\n",
    "                            if existing_answer:\n",
    "                                questions[idx][\"answer\"] = existing_answer + \" \" + answer.strip()\n",
    "                            else:\n",
    "                                questions[idx][\"answer\"] = answer.strip()\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing response: {e}\")\n",
    "\n",
    "    # Save the updated questions back to the same JSON file\n",
    "    with open(questions_file, \"w\") as file:\n",
    "        json.dump(questions, file, indent=4)\n",
    "\n",
    "    print(f\"Answers have been updated and saved back to {questions_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/3...\n",
      "Processing chunk 2/3...\n",
      "Processing chunk 3/3...\n",
      "Answers have been updated and saved back to questions/mbon_questions.json.\n"
     ]
    }
   ],
   "source": [
    "# Path to your PDF file\n",
    "pdf_path = \"papers/elife-62576-v2.pdf\"\n",
    "\n",
    "# Extract text from the PDF\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Split the text into manageable chunks\n",
    "chunks = chunk_text(pdf_text)\n",
    "# Initialize the GPT_Model\n",
    "gpt_model = GPT_Model(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    api_key=openai.api_key,\n",
    "    system_prompt=\"You are a helpful assistant for a research scientist. You have been given a research paper and asked to answer questions based on the content.\",\n",
    ")\n",
    "\n",
    "# Load questions from a JSON file\n",
    "questions_json_file = \"questions/mbon_questions.json\"\n",
    "ask_questions_and_fill_answers_2(chunks, questions_json_file, gpt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/3...\n",
      "Processing chunk 2/3...\n",
      "Processing chunk 3/3...\n",
      "Error parsing response: invalid literal for int() with base 10: 'The provided text does not contain specific information about MBON01 through MBON35'\n",
      "Answers have been updated and saved back to questions/mbon_summary.json.\n"
     ]
    }
   ],
   "source": [
    "# Path to your PDF file\n",
    "pdf_path = \"papers/elife-62576-v2.pdf\"\n",
    "\n",
    "# Extract text from the PDF\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Split the text into manageable chunks\n",
    "chunks = chunk_text(pdf_text)\n",
    "# Initialize the GPT_Model\n",
    "gpt_model = GPT_Model(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=openai.api_key,\n",
    "    system_prompt=\"You are a helpful assistant for a research scientist. You have been given a research paper and asked to answer questions based on the content.\",\n",
    ")\n",
    "\n",
    "# Load questions from a JSON file\n",
    "questions_json_file = \"questions/mbon_summary.json\"\n",
    "ask_questions_and_fill_answers_2(chunks, questions_json_file, gpt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm201",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
